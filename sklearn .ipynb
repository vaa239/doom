{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_res = pd.read_csv(\"data.csv\", header=None)\n",
    "import pickle\n",
    "images = pickle.loads(df_res.loc[0,1])\n",
    "index = pickle.loads(df_res.loc[0,2])\n",
    "actions = pickle.loads(df_res.loc[0,3])\n",
    "actions_new = pickle.loads(df_res.loc[0,4])\n",
    "sub_action = actions_new[actions_new[:,0] == 0, :]\n",
    "sub_image = images[:,:,actions_new[:,0] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "sub_image = pickle.load( open( \"images1.txt\", \"rb\" ) )\n",
    "sub_action = pickle.load( open( \"actions1.txt\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(x): return x >30 and x < 60 and x%4 != 0 #x % 3 == 0 or x % 5 == 0\n",
    "ind = filter(f, range(80))\n",
    "def g(x): return x%2 != 0 #x % 3 == 0 or x % 5 == 0\n",
    "ind_w = filter(g, range(80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(x): return x%7== 0 #x % 3 == 0 or x % 5 == 0\n",
    "ind7 = filter(f, range(sub_action.shape[0]-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ind7[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(images[ind,:,239], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cropped_tmp = images[ind,:,:]\n",
    "cropped = cropped_tmp[:,ind_w,:]\n",
    "# plt.imshow(cropped[:,ind_w], cmap='gray')\n",
    "cropped_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(cropped[:,:,1000], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(images[:,:,1000], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = sub_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_target = np.zeros(y.shape[0])\n",
    "for i in range(y_target.shape[0]):\n",
    "    y_target[i]= sub_action[i].nonzero()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 80, 6156)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = sub_image\n",
    "cropped_tmp = X[ind,:,:]\n",
    "cropped = cropped_tmp[:,ind_w,:]\n",
    "\n",
    "x_turned =cropped.transpose(2,0,1).reshape(y_target.shape[0],-1)\n",
    "# x_turned = x_turned[ind7]\n",
    "# x_turned = np.rot90(np.flip(np.swapaxes(np.swapaxes(cropped, 2,0), 0,1),1),k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_turned[ind7].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits= 5, shuffle= True)\n",
    "\n",
    "clf = LogisticRegressionCV()\n",
    "\n",
    "cv = kfold.split(x_turned[ind7], y_target[ind7])\n",
    "\n",
    "scores = cross_val_score(clf, x_turned[ind7], y_target[ind7], scoring='accuracy', cv=cv)\n",
    "\n",
    "print ('StratifiedKFold score: {0:.3f} Â± {1:.3f}'.format(scores.mean(), scores.std()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1833, 3274, 1655, ..., 1050,   36, 1638])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(sub_action))\n",
    "dim = len(sub_action)\n",
    "arr = np.arange(dim)\n",
    "np.random.shuffle(arr)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(879,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = len(sub_action)\n",
    "arr = np.arange(dim)\n",
    "arr = arr[:len(sub_action)/7]\n",
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_turned[arr], y_target[arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LR = LogisticRegression(multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00909090909090909"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(np.sum(LR.predict(x_test) != y_test))/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=device=cpu,floatX=float32\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS=device=cpu,floatX=float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-03-21 10:02:49,730] Making new env: ppaquette/DoomBasic-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ppaquette_gym_doom\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.wrappers import SkipWrapper\n",
    "from ppaquette_gym_doom.wrappers.action_space import ToDiscrete\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "GAME_NAME = 'ppaquette/DoomBasic-v0'\n",
    "\n",
    "env = ToDiscrete('minimal')(gym.make(GAME_NAME))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'toimage' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-de63e9a73cf1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdef_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdef_state_80_rg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoimage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdef_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mANTIALIAS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'L'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mds80_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdef_state_80_rg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'toimage' is not defined"
     ]
    }
   ],
   "source": [
    "def_state = env.reset()\n",
    "\n",
    "def_state_80_rg = np.array(toimage(def_state).resize((80,80), Image.ANTIALIAS).convert('L'))\n",
    "\n",
    "ds80_1 = def_state_80_rg[ind,:]\n",
    "ds80_2 = ds80_1[:,ind_w]\n",
    "LR.predict(ds80_2.reshape(1,-1))\n",
    "new_state = def_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_state = np.array(toimage(new_state).resize((80,80), Image.ANTIALIAS).convert('L'))\n",
    "processed_state = processed_state[ind,:]\n",
    "processed_state = processed_state[:,ind_w]\n",
    "processed_state = processed_state.reshape(1,-1)\n",
    "action = LR.predict(processed_state)[0]\n",
    "print action\n",
    "new_state = env.step(int(action))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def_state = env.reset()\n",
    "\n",
    "i = 10\n",
    "if i % 4 == 2:\n",
    "    print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "episode finished!,0\n",
      "total reward: 70.0, time:0.300343990326\n",
      "episode finished!,1\n",
      "total reward: 78.0, time:0.190007925034\n",
      "episode finished!,2\n",
      "total reward: -470.0, time:2.56851100922\n",
      "episode finished!,3\n",
      "total reward: 76.0, time:0.249701023102\n",
      "episode finished!,4\n",
      "total reward: -465.0, time:2.36446094513\n",
      "episode finished!,5\n",
      "total reward: 68.0, time:0.183190107346\n",
      "episode finished!,6\n",
      "total reward: 75.0, time:0.145995140076\n",
      "episode finished!,7\n",
      "total reward: 71.0, time:0.177366018295\n",
      "episode finished!,8\n",
      "total reward: 93.0, time:0.0526051521301\n",
      "episode finished!,9\n",
      "total reward: 65.0, time:0.199979066849\n",
      "episode finished!,10\n",
      "total reward: -470.0, time:1.89336705208\n",
      "episode finished!,11\n",
      "total reward: 84.0, time:0.0987911224365\n",
      "episode finished!,12\n",
      "total reward: 94.0, time:0.0457670688629\n",
      "episode finished!,13\n",
      "total reward: 85.0, time:0.0906870365143\n",
      "episode finished!,14\n",
      "total reward: 90.0, time:0.0675699710846\n",
      "episode finished!,15\n",
      "total reward: 76.0, time:0.141279935837\n",
      "episode finished!,16\n",
      "total reward: 86.0, time:0.0885200500488\n",
      "episode finished!,17\n",
      "total reward: 70.0, time:0.172868967056\n",
      "episode finished!,18\n",
      "total reward: 93.0, time:0.0509300231934\n",
      "episode finished!,19\n",
      "total reward: 71.0, time:0.168678045273\n",
      "episode finished!,20\n",
      "total reward: 95.0, time:0.0394310951233\n",
      "episode finished!,21\n",
      "total reward: -470.0, time:1.88535404205\n",
      "episode finished!,22\n",
      "total reward: 74.0, time:0.151890993118\n",
      "episode finished!,23\n",
      "total reward: -465.0, time:1.90599989891\n",
      "episode finished!,24\n",
      "total reward: 85.0, time:0.0993931293488\n",
      "episode finished!,25\n",
      "total reward: -470.0, time:1.9074780941\n",
      "episode finished!,26\n",
      "total reward: 76.0, time:0.153909921646\n",
      "episode finished!,27\n",
      "total reward: 68.0, time:0.164188861847\n",
      "episode finished!,28\n",
      "total reward: 85.0, time:0.0935940742493\n",
      "episode finished!,29\n",
      "total reward: 82.0, time:0.109889030457\n",
      "episode finished!,30\n",
      "total reward: 18.0, time:0.368731975555\n",
      "episode finished!,31\n",
      "total reward: 75.0, time:0.146941900253\n",
      "episode finished!,32\n",
      "total reward: 82.0, time:0.109961032867\n",
      "episode finished!,33\n",
      "total reward: 22.0, time:0.373899936676\n",
      "episode finished!,34\n",
      "total reward: 75.0, time:0.147166013718\n",
      "episode finished!,35\n",
      "total reward: 67.0, time:0.309689998627\n",
      "episode finished!,36\n",
      "total reward: 84.0, time:0.148153066635\n",
      "episode finished!,37\n",
      "total reward: 70.0, time:0.198777914047\n",
      "episode finished!,38\n",
      "total reward: 95.0, time:0.0478439331055\n",
      "episode finished!,39\n",
      "total reward: 63.0, time:0.20090508461\n",
      "episode finished!,40\n",
      "total reward: 61.0, time:0.28161907196\n",
      "episode finished!,41\n",
      "total reward: 75.0, time:0.195698022842\n",
      "episode finished!,42\n",
      "total reward: 85.0, time:0.122153997421\n",
      "episode finished!,43\n",
      "total reward: 67.0, time:0.209003925323\n",
      "episode finished!,44\n",
      "total reward: 65.0, time:0.213668823242\n",
      "episode finished!,45\n",
      "total reward: 23.0, time:0.496994018555\n",
      "episode finished!,46\n",
      "total reward: 83.0, time:0.137789011002\n",
      "episode finished!,47\n",
      "total reward: 84.0, time:0.1091401577\n",
      "episode finished!,48\n",
      "total reward: 75.0, time:0.182127952576\n",
      "episode finished!,49\n",
      "total reward: 77.0, time:0.149304151535\n",
      "episode finished!,50\n",
      "total reward: 85.0, time:0.108622074127\n",
      "episode finished!,51\n",
      "total reward: 65.0, time:0.319926023483\n",
      "episode finished!,52\n",
      "total reward: 93.0, time:0.0639638900757\n",
      "episode finished!,53\n",
      "total reward: 91.0, time:0.0709571838379\n",
      "episode finished!,54\n",
      "total reward: 77.0, time:0.151509046555\n",
      "episode finished!,55\n",
      "total reward: 95.0, time:0.046914100647\n",
      "episode finished!,56\n",
      "total reward: 76.0, time:0.147338151932\n",
      "episode finished!,57\n",
      "total reward: 67.0, time:0.308437108994\n",
      "episode finished!,58\n",
      "total reward: 77.0, time:0.194545030594\n",
      "episode finished!,59\n",
      "total reward: 94.0, time:0.049576997757\n",
      "episode finished!,60\n",
      "total reward: 90.0, time:0.0683889389038\n",
      "episode finished!,61\n",
      "total reward: 85.0, time:0.0988609790802\n",
      "episode finished!,62\n",
      "total reward: 76.0, time:0.147855043411\n",
      "episode finished!,63\n",
      "total reward: 93.0, time:0.0528428554535\n",
      "episode finished!,64\n",
      "total reward: 71.0, time:0.186378955841\n",
      "episode finished!,65\n",
      "total reward: 71.0, time:0.185271024704\n",
      "episode finished!,66\n",
      "total reward: 75.0, time:0.185563087463\n",
      "episode finished!,67\n",
      "total reward: 90.0, time:0.0877161026001\n",
      "episode finished!,68\n",
      "total reward: 95.0, time:0.0572171211243\n",
      "episode finished!,69\n",
      "total reward: 65.0, time:0.214442968369\n",
      "episode finished!,70\n",
      "total reward: 74.0, time:0.196146965027\n",
      "episode finished!,71\n",
      "total reward: 65.0, time:0.258322954178\n",
      "episode finished!,72\n",
      "total reward: 65.0, time:0.245734930038\n",
      "episode finished!,73\n",
      "total reward: 85.0, time:0.123370885849\n",
      "episode finished!,74\n",
      "total reward: 73.0, time:0.247853040695\n",
      "episode finished!,75\n",
      "total reward: 82.0, time:0.185292005539\n",
      "episode finished!,76\n",
      "total reward: 85.0, time:0.134020090103\n",
      "episode finished!,77\n",
      "total reward: 95.0, time:0.0453028678894\n",
      "episode finished!,78\n",
      "total reward: 65.0, time:0.213382005692\n",
      "episode finished!,79\n",
      "total reward: 68.0, time:0.191439151764\n",
      "episode finished!,80\n",
      "total reward: 93.0, time:0.0528879165649\n",
      "episode finished!,81\n",
      "total reward: -465.0, time:1.90108418465\n",
      "episode finished!,82\n",
      "total reward: 85.0, time:0.0949311256409\n",
      "episode finished!,83\n",
      "total reward: -174.0, time:1.17122793198\n",
      "episode finished!,84\n",
      "total reward: 82.0, time:0.112899065018\n",
      "episode finished!,85\n",
      "total reward: 74.0, time:0.161325931549\n",
      "episode finished!,86\n",
      "total reward: 68.0, time:0.195500135422\n",
      "episode finished!,87\n",
      "total reward: 68.0, time:0.185230970383\n",
      "episode finished!,88\n",
      "total reward: 84.0, time:0.0997910499573\n",
      "episode finished!,89\n",
      "total reward: 75.0, time:0.147768974304\n",
      "episode finished!,90\n",
      "total reward: 84.0, time:0.100363969803\n",
      "episode finished!,91\n",
      "total reward: 95.0, time:0.0389029979706\n",
      "episode finished!,92\n",
      "total reward: 85.0, time:0.0941469669342\n",
      "episode finished!,93\n",
      "total reward: 95.0, time:0.0393581390381\n",
      "episode finished!,94\n",
      "total reward: 49.0, time:0.230820178986\n",
      "episode finished!,95\n",
      "total reward: 95.0, time:0.0393569469452\n",
      "episode finished!,96\n",
      "total reward: 45.0, time:0.252892017365\n",
      "episode finished!,97\n",
      "total reward: 84.0, time:0.103888988495\n",
      "episode finished!,98\n",
      "total reward: 75.0, time:0.157948970795\n",
      "episode finished!,99\n",
      "total reward: -470.0, time:1.95027589798\n",
      "episode finished!,100\n",
      "total reward: 75.0, time:0.147926092148\n",
      "episode finished!,101\n",
      "total reward: 95.0, time:0.0386250019073\n",
      "episode finished!,102\n",
      "total reward: -470.0, time:1.8799469471\n",
      "episode finished!,103\n",
      "total reward: -470.0, time:1.87749695778\n",
      "episode finished!,104\n",
      "total reward: 65.0, time:0.199884176254\n",
      "episode finished!,105\n",
      "total reward: 95.0, time:0.0390520095825\n",
      "episode finished!,106\n",
      "total reward: 75.0, time:0.145974874496\n",
      "episode finished!,107\n",
      "total reward: 93.0, time:0.0502469539642\n",
      "episode finished!,108\n",
      "total reward: 75.0, time:0.149139165878\n",
      "episode finished!,109\n",
      "total reward: 76.0, time:0.14167881012\n",
      "episode finished!,110\n",
      "total reward: 76.0, time:0.14165186882\n",
      "episode finished!,111\n",
      "total reward: -44.0, time:0.617232084274\n",
      "episode finished!,112\n",
      "total reward: 83.0, time:0.105660915375\n",
      "episode finished!,113\n",
      "total reward: 85.0, time:0.0948879718781\n",
      "episode finished!,114\n",
      "total reward: 23.0, time:0.38933300972\n",
      "episode finished!,115\n",
      "total reward: 26.0, time:0.392654895782\n",
      "episode finished!,116\n",
      "total reward: -470.0, time:1.90570092201\n",
      "episode finished!,117\n",
      "total reward: 75.0, time:0.147624969482\n",
      "episode finished!,118\n",
      "total reward: 65.0, time:0.174530029297\n",
      "episode finished!,119\n",
      "total reward: 75.0, time:0.14762711525\n",
      "episode finished!,120\n",
      "total reward: 68.0, time:0.18452501297\n",
      "episode finished!,121\n",
      "total reward: 75.0, time:0.147797107697\n",
      "episode finished!,122\n",
      "total reward: 69.0, time:0.152907133102\n",
      "episode finished!,123\n",
      "total reward: 73.0, time:0.158510923386\n",
      "episode finished!,124\n",
      "total reward: 94.0, time:0.0449190139771\n",
      "episode finished!,125\n",
      "total reward: 91.0, time:0.0614218711853\n",
      "episode finished!,126\n",
      "total reward: 67.0, time:0.189663171768\n",
      "episode finished!,127\n",
      "total reward: -465.0, time:1.92107200623\n",
      "episode finished!,128\n",
      "total reward: 71.0, time:0.176820993423\n",
      "episode finished!,129\n",
      "total reward: 95.0, time:0.0414669513702\n",
      "episode finished!,130\n",
      "total reward: 82.0, time:0.10942196846\n",
      "episode finished!,131\n",
      "total reward: -132.0, time:1.0111041069\n",
      "episode finished!,132\n",
      "total reward: 75.0, time:0.151765108109\n",
      "episode finished!,133\n",
      "total reward: 93.0, time:0.0523171424866\n",
      "episode finished!,134\n",
      "total reward: -470.0, time:1.90360879898\n",
      "episode finished!,135\n",
      "total reward: -465.0, time:1.89251804352\n",
      "episode finished!,136\n",
      "total reward: 76.0, time:0.14253783226\n",
      "episode finished!,137\n",
      "total reward: 77.0, time:0.136536836624\n",
      "episode finished!,138\n",
      "total reward: 75.0, time:0.147783994675\n",
      "episode finished!,139\n",
      "total reward: 74.0, time:0.152964115143\n",
      "episode finished!,140\n",
      "total reward: 65.0, time:0.199974060059\n",
      "episode finished!,141\n",
      "total reward: 95.0, time:0.0397419929504\n",
      "episode finished!,142\n",
      "total reward: 71.0, time:0.167320013046\n",
      "episode finished!,143\n",
      "total reward: -6.0, time:0.469442129135\n",
      "episode finished!,144\n",
      "total reward: 95.0, time:0.0384111404419\n",
      "episode finished!,145\n",
      "total reward: 66.0, time:0.19390296936\n",
      "episode finished!,146\n",
      "total reward: 71.0, time:0.168248176575\n",
      "episode finished!,147\n",
      "total reward: 45.0, time:0.25294303894\n",
      "episode finished!,148\n",
      "total reward: 75.0, time:0.147160053253\n",
      "episode finished!,149\n",
      "total reward: 83.0, time:0.105328798294\n",
      "episode finished!,150\n",
      "total reward: 92.0, time:0.0549919605255\n",
      "episode finished!,151\n",
      "total reward: 67.0, time:0.192616939545\n",
      "episode finished!,152\n",
      "total reward: 74.0, time:0.152854919434\n",
      "episode finished!,153\n",
      "total reward: 3.0, time:0.448118925095\n",
      "episode finished!,154\n",
      "total reward: 93.0, time:0.0501770973206\n",
      "episode finished!,155\n",
      "total reward: 70.0, time:0.175285100937\n",
      "episode finished!,156\n",
      "total reward: 75.0, time:0.147734165192\n",
      "episode finished!,157\n",
      "total reward: 70.0, time:0.173940181732\n",
      "episode finished!,158\n",
      "total reward: 75.0, time:0.147578954697\n",
      "episode finished!,159\n",
      "total reward: 75.0, time:0.147506952286\n",
      "episode finished!,160\n",
      "total reward: 85.0, time:0.0948469638824\n",
      "episode finished!,161\n",
      "total reward: 75.0, time:0.147343873978\n",
      "episode finished!,162\n",
      "total reward: 83.0, time:0.10523891449\n",
      "episode finished!,163\n",
      "total reward: 93.0, time:0.0502750873566\n",
      "episode finished!,164\n",
      "total reward: 74.0, time:0.154788017273\n",
      "episode finished!,165\n",
      "total reward: 95.0, time:0.0391111373901\n",
      "episode finished!,166\n",
      "total reward: 84.0, time:0.098375082016\n",
      "episode finished!,167\n",
      "total reward: 65.0, time:0.199579000473\n",
      "episode finished!,168\n",
      "total reward: 76.0, time:0.14209485054\n",
      "episode finished!,169\n",
      "total reward: 93.0, time:0.0494101047516\n",
      "episode finished!,170\n",
      "total reward: 75.0, time:0.146440029144\n",
      "episode finished!,171\n",
      "total reward: 82.0, time:0.110720872879\n",
      "episode finished!,172\n",
      "total reward: 71.0, time:0.168110132217\n",
      "episode finished!,173\n",
      "total reward: -119.0, time:0.905630111694\n",
      "episode finished!,174\n",
      "total reward: -470.0, time:1.84607195854\n",
      "episode finished!,175\n",
      "total reward: 67.0, time:0.189509153366\n",
      "episode finished!,176\n",
      "total reward: 75.0, time:0.147058010101\n",
      "episode finished!,177\n",
      "total reward: 76.0, time:0.141899108887\n",
      "episode finished!,178\n",
      "total reward: 68.0, time:0.183442831039\n",
      "episode finished!,179\n",
      "total reward: 74.0, time:0.152117967606\n",
      "episode finished!,180\n",
      "total reward: 76.0, time:0.141986131668\n",
      "episode finished!,181\n",
      "total reward: 85.0, time:0.0942831039429\n",
      "episode finished!,182\n",
      "total reward: 70.0, time:0.148136138916\n",
      "episode finished!,183\n",
      "total reward: 92.0, time:0.0552799701691\n",
      "episode finished!,184\n",
      "total reward: 71.0, time:0.170356988907\n",
      "episode finished!,185\n",
      "total reward: 95.0, time:0.0387060642242\n",
      "episode finished!,186\n",
      "total reward: 75.0, time:0.146792888641\n",
      "episode finished!,187\n",
      "total reward: 93.0, time:0.0507071018219\n",
      "episode finished!,188\n",
      "total reward: 74.0, time:0.153902053833\n",
      "episode finished!,189\n",
      "total reward: -470.0, time:1.84770417213\n",
      "episode finished!,190\n",
      "total reward: 68.0, time:0.184515953064\n",
      "episode finished!,191\n",
      "total reward: 76.0, time:0.141642093658\n",
      "episode finished!,192\n",
      "total reward: 94.0, time:0.0434629917145\n",
      "episode finished!,193\n",
      "total reward: 71.0, time:0.168110132217\n",
      "episode finished!,194\n",
      "total reward: 76.0, time:0.142098903656\n",
      "episode finished!,195\n",
      "total reward: 65.0, time:0.174311161041\n",
      "episode finished!,196\n",
      "total reward: 51.0, time:0.221767902374\n",
      "episode finished!,197\n",
      "total reward: 76.0, time:0.142002105713\n",
      "episode finished!,198\n",
      "total reward: -465.0, time:1.85065889359\n",
      "episode finished!,199\n",
      "total reward: 68.0, time:0.184041976929\n",
      "25.52\n",
      "153.7967802\n",
      "0.184041976929\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 200\n",
    "#     index = np.empty([1, 1])\n",
    "import time\n",
    "from scipy.misc import toimage\n",
    "from PIL import Image\n",
    "rew = []\n",
    "ti = []\n",
    "\n",
    "for j in range(1):\n",
    "    print(\"\")\n",
    "    rewards = []\n",
    "    time_val = []\n",
    "    for i in range(episodes):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "#         if i+1 % 25 == 2:\n",
    "#             print(\"episode #\", i)\n",
    "\n",
    "        def_state = env.reset()\n",
    "        is_done = False\n",
    "\n",
    "        def_state_80_rg = np.array(toimage(def_state).resize((80,80), Image.ANTIALIAS).convert('L'))\n",
    "\n",
    "        ds80_1 = def_state_80_rg[ind,:]\n",
    "        ds80_2 = ds80_1[:,ind_w]\n",
    "        action = LR.predict(ds80_2.reshape(1,-1))\n",
    "        tot_rew = 0\n",
    "\n",
    "        st_num = 0\n",
    "    #     game.new_episode()\n",
    "        while not is_done:\n",
    "\n",
    "            st_num += 1\n",
    "            new_env = env.step(int(action))\n",
    "            new_state = new_env[0]\n",
    "            is_done = new_env[2]\n",
    "\n",
    "            r = new_env[1]\n",
    "\n",
    "            processed_state = np.array(toimage(new_state).resize((80,80), Image.ANTIALIAS).convert('L'))\n",
    "            processed_state = processed_state[ind,:]\n",
    "            processed_state = processed_state[:,ind_w]\n",
    "            processed_state = processed_state.reshape(1,-1)\n",
    "            action = LR.predict(processed_state)[0]\n",
    "\n",
    "            tot_rew += r\n",
    "\n",
    "\n",
    "\n",
    "    #         game.advance_action()\n",
    "    #         a = game.get_last_action()\n",
    "    #         r = game.get_last_reward()\n",
    "\n",
    "    #         if (s.number % frame_sk == 0):\n",
    "    #             im_ar = np.array(toimage(img).resize((80,80), Image.ANTIALIAS).convert('L'))\n",
    "    #             images = np.dstack((images, im_ar))\n",
    "    #             actions = np.append(actions, np.atleast_2d(a), axis=0)\n",
    "    #             rew_step.append(r)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #         imgs.append(img)\n",
    "    #         actions.append(a)\n",
    "\n",
    "#             print(\"state #\", st_num)\n",
    "#             #print(\"game variables: \", misc)\n",
    "#             print(\"action:\", action)\n",
    "#             print(\"reward:\",r)\n",
    "#             print(\"=====================\")\n",
    "\n",
    "        ep_time = time.time() - start_time\n",
    "        print(\"episode finished!,{}\".format(i))\n",
    "        print(\"total reward: {}, time:{}\".format(tot_rew,ep_time))\n",
    "\n",
    "        rewards.append(tot_rew)\n",
    "        time_val.append(ep_time)\n",
    "    #     print(\"************************\")\n",
    "    print np.mean(rewards)\n",
    "    print np.std(rewards)\n",
    "    print np.mean(ep_time)\n",
    "    print np.std(ep_time)\n",
    "    ti.append([np.mean(ep_time), np.std(ep_time)])\n",
    "    rew.append([np.mean(rewards), np.std(rewards)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.mean(rewards)\n",
    "print np.std(rewards)\n",
    "\n",
    "np.swapaxes(np.asarray(rew),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(np.asarray(rew), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "st = env.step(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(decision_function_shape='ovr',kernel = 'linear', C = 0.000001)\n",
    "clf.fit(x_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(clf.predict(x_test) != y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svr = svm.SVC(decision_function_shape='ovr')\n",
    "clf = GridSearchCV(svr, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(x_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(clf.predict(x_test) != y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
