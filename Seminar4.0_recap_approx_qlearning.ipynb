{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate q-learning\n",
    "\n",
    "In this notebook you will teach a lasagne neural network to do Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. For example, it translates to TensorFlow almost line-to-line. However, we recommend you to stick to theano/lasagne unless you're certain about your skills in the framework of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS='floatX=float32'\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 16:00:47,615] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdcb2247048>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEftJREFUeJzt3XGsnXddx/H3x24MAtNt7trUtnMlqSYd0Q5uKgZiJguu\nTmPhn6Ukkv4xc/dHJaAk2kmi8EcTNAL+NUKRaaNIbQRcs6CmqzOERFfuoBttt7Ir69I2XXsBCcw/\nii1f/zhP2bHc3nvuPff07vx4v5KT8zy/53nO+X7T5nOf+9znd06qCklSe35ipQuQJI2GAS9JjTLg\nJalRBrwkNcqAl6RGGfCS1KiRBXySrUlOJJlJsmtU7yNJmltGcR98klXA14G3A6eBLwPvqqrjy/5m\nkqQ5jeoMfgswU1XfqKrvA/uAbSN6L0nSHK4b0euuBU71rZ8GfvlqO9966611++23j6gUSRo/J0+e\n5Jvf/GaGeY1RBfyCkkwBUwC33XYb09PTK1WKJL3iTE5ODv0ao7pEcwZY37e+rhv7oaraU1WTVTU5\nMTExojIk6cfXqAL+y8DGJBuSvArYDhwY0XtJkuYwkks0VXUxye8B/wqsAh6uqmOjeC9J0txGdg2+\nqr4AfGFUry9Jmp8zWSWpUQa8JDXKgJekRhnwktQoA16SGmXAS1KjDHhJapQBL0mNMuAlqVEGvCQ1\nyoCXpEYZ8JLUKANekhplwEtSowx4SWqUAS9JjTLgJalRBrwkNWqor+xLchL4HnAJuFhVk0luAf4B\nuB04CdxXVf89XJmSpMVajjP4X6uqzVU12a3vAg5V1UbgULcuSbrGRnGJZhuwt1veC7xjBO8hSVrA\nsAFfwGNJnkwy1Y2trqqz3fKLwOoh30OStARDXYMH3lpVZ5L8DHAwybP9G6uqktRcB3Y/EKYAbrvt\ntiHLkCRdaagz+Ko60z2fBz4PbAHOJVkD0D2fv8qxe6pqsqomJyYmhilDkjSHJQd8ktcmufHyMvDr\nwFHgALCj220H8MiwRUqSFm+YSzSrgc8nufw6f19V/5Lky8D+JPcDLwD3DV+mJGmxlhzwVfUN4Jfm\nGP8WcPcwRUmShudMVklqlAEvSY0y4CWpUQa8JDXKgJekRhnwktQoA16SGmXAS1KjDHhJapQBL0mN\nMuAlqVEGvCQ1yoCXpEYZ8JLUKANekhplwEtSowx4SWqUAS9JjTLgJalRCwZ8koeTnE9ytG/sliQH\nkzzXPd/ct+3BJDNJTiS5Z1SFS5LmN8gZ/N8AW68Y2wUcqqqNwKFunSSbgO3AHd0xDyVZtWzVSpIG\ntmDAV9UXgW9fMbwN2Nst7wXe0Te+r6ouVNXzwAywZZlqlSQtwlKvwa+uqrPd8ovA6m55LXCqb7/T\n3diPSDKVZDrJ9Ozs7BLLkCRdzdB/ZK2qAmoJx+2pqsmqmpyYmBi2DEnSFZYa8OeSrAHons9342eA\n9X37revGJEnX2FID/gCwo1veATzSN749yQ1JNgAbgcPDlShJWorrFtohyWeAu4Bbk5wG/hT4MLA/\nyf3AC8B9AFV1LMl+4DhwEdhZVZdGVLskaR4LBnxVvesqm+6+yv67gd3DFCVJGp4zWSWpUQa8JDXK\ngJekRhnwktQoA16SGmXAS1KjDHhJapQBL0mNMuAlqVEGvCQ1yoCXpEYZ8JLUKANekhplwEtSowx4\nSWqUAS9JjTLgJalRBrwkNWrBgE/ycJLzSY72jX0wyZkkR7rHvX3bHkwyk+REkntGVbgkaX6DnMH/\nDbB1jvGPVdXm7vEFgCSbgO3AHd0xDyVZtVzFSpIGt2DAV9UXgW8P+HrbgH1VdaGqngdmgC1D1CdJ\nWqJhrsG/J8nT3SWcm7uxtcCpvn1Od2M/IslUkukk07Ozs0OUIUmay1ID/uPA64HNwFngI4t9gara\nU1WTVTU5MTGxxDIkSVezpICvqnNVdamqfgB8kpcvw5wB1vftuq4bkyRdY0sK+CRr+lbfCVy+w+YA\nsD3JDUk2ABuBw8OVKElaiusW2iHJZ4C7gFuTnAb+FLgryWaggJPAAwBVdSzJfuA4cBHYWVWXRlO6\nJGk+CwZ8Vb1rjuFPzbP/bmD3MEVJkobnTFZJapQBL0mNMuAlqVEGvCQ1yoCXpEYZ8JLUqAVvk5Ra\n9OSeB35k7E1Tn1iBSqTR8QxekhplwEtSowx4SWqUAS9JjTLgJalRBrwkNcqAl6RGGfCS1CgDXpIa\nZcBLUqMMeP1YmutjCeb6+AJpnC0Y8EnWJ3k8yfEkx5K8txu/JcnBJM91zzf3HfNgkpkkJ5LcM8oG\nJElzG+QM/iLw/qraBLwZ2JlkE7ALOFRVG4FD3Trdtu3AHcBW4KEkq0ZRvCTp6hYM+Ko6W1Vf6Za/\nBzwDrAW2AXu73fYC7+iWtwH7qupCVT0PzABblrtwSdL8FnUNPsntwJ3AE8DqqjrbbXoRWN0trwVO\n9R12uhu78rWmkkwnmZ6dnV1k2ZKkhQwc8EleB3wWeF9Vfbd/W1UVUIt546raU1WTVTU5MTGxmEMl\nSQMYKOCTXE8v3D9dVZ/rhs8lWdNtXwOc78bPAOv7Dl/XjUmSrqFB7qIJ8Cngmar6aN+mA8CObnkH\n8Ejf+PYkNyTZAGwEDi9fyZKkQQzylX1vAd4NfC3JkW7sj4EPA/uT3A+8ANwHUFXHkuwHjtO7A2dn\nVV1a9solSfNaMOCr6ktArrL57qscsxvYPURdkqQhOZNVkhplwEtSowx4SWqUAS9JjTLgJalRBrwk\nNcqAl6RGGfCS1CgDXpIaZcBLUqMMeElqlAEvSY0y4CWpUQa8JDXKgJekRhnwktQoA16SGmXAS1Kj\nBvnS7fVJHk9yPMmxJO/txj+Y5EySI93j3r5jHkwyk+REkntG2YAkaW6DfOn2ReD9VfWVJDcCTyY5\n2G37WFX9Rf/OSTYB24E7gJ8FHkvy837xtiRdWwuewVfV2ar6Srf8PeAZYO08h2wD9lXVhap6HpgB\ntixHsZKkwS3qGnyS24E7gSe6ofckeTrJw0lu7sbWAqf6DjvN/D8QJEkjMHDAJ3kd8FngfVX1XeDj\nwOuBzcBZ4COLeeMkU0mmk0zPzs4u5lBJ0gAGCvgk19ML909X1ecAqupcVV2qqh8An+TlyzBngPV9\nh6/rxv6fqtpTVZNVNTkxMTFMD5KkOQxyF02ATwHPVNVH+8bX9O32TuBot3wA2J7khiQbgI3A4eUr\nWZI0iEHO4N8CvBt42xW3RP55kq8leRr4NeD3AarqGLAfOA78C7DTO2j0SvSmqU/8yNiTex5YgUqk\n0VjwNsmq+hKQOTZ9YZ5jdgO7h6hLkjQkZ7JKUqMMeElqlAEvSY0y4CWpUQa8JDXKgJekRhnwktQo\nA16SGmXAS1KjDHhJapQBL0mNMuAlqVEGvCQ1yoBXc5IM/BjF8dIrhQEvSY1a8PPgpdY9enbqh8u/\ntWbPClYiLS/P4PVjrT/cL69Pf2LqKntL48WAl65wZehL42qQL91+dZLDSZ5KcizJh7rxW5IcTPJc\n93xz3zEPJplJciLJPaNsQFpuXqZRKwY5g78AvK2qfgnYDGxN8mZgF3CoqjYCh7p1kmwCtgN3AFuB\nh5KsGkXx0rCuDHPDXS0Z5Eu3C3ipW72+exSwDbirG98L/DvwR934vqq6ADyfZAbYAvzHchYuLYfJ\nB/YAL4f6B1esEmn5DXQNPsmqJEeA88DBqnoCWF1VZ7tdXgRWd8trgVN9h5/uxiRJ19BAAV9Vl6pq\nM7AO2JLkDVdsL3pn9QNLMpVkOsn07OzsYg6VJA1gUXfRVNV3gMfpXVs/l2QNQPd8vtvtDLC+77B1\n3diVr7WnqiaranJiYmIptUuS5jHIXTQTSW7qll8DvB14FjgA7Oh22wE80i0fALYnuSHJBmAjcHi5\nC5ckzW+QmaxrgL3dnTA/AeyvqkeT/AewP8n9wAvAfQBVdSzJfuA4cBHYWVWXRlO+JOlqBrmL5mng\nzjnGvwXcfZVjdgO7h65OkrRkzmSVpEYZ8JLUKANekhrlxwWrOb1pGZI8g5ekRhnwktQoA16SGmXA\nS1KjDHhJapQBL0mNMuAlqVEGvCQ1yoCXpEYZ8JLUKANekhplwEtSowx4SWqUAS9JjRrkS7dfneRw\nkqeSHEvyoW78g0nOJDnSPe7tO+bBJDNJTiS5Z5QNSJLmNsjnwV8A3lZVLyW5HvhSkn/utn2sqv6i\nf+ckm4DtwB3AzwKPJfl5v3hbkq6tBc/gq+elbvX67jHfNypsA/ZV1YWqeh6YAbYMXakkaVEGugaf\nZFWSI8B54GBVPdFtek+Sp5M8nOTmbmwtcKrv8NPdmCTpGhoo4KvqUlVtBtYBW5K8Afg48HpgM3AW\n+Mhi3jjJVJLpJNOzs7OLLFuStJBF3UVTVd8BHge2VtW5Lvh/AHySly/DnAHW9x22rhu78rX2VNVk\nVU1OTEwsrXpJ0lUNchfNRJKbuuXXAG8Hnk2ypm+3dwJHu+UDwPYkNyTZAGwEDi9v2ZKkhQxyF80a\nYG+SVfR+IOyvqkeT/G2SzfT+4HoSeACgqo4l2Q8cBy4CO72DRpKuvQUDvqqeBu6cY/zd8xyzG9g9\nXGmSpGE4k1WSGmXAS1KjDHhJapQBL0mNMuAlqVEGvCQ1yoCXpEYZ8JLUKANekhplwEtSowx4SWqU\nAS9JjTLgJalRBrwkNcqAl6RGGfCS1CgDXpIaZcBLUqMMeElq1MABn2RVkq8mebRbvyXJwSTPdc83\n9+37YJKZJCeS3DOKwiVJ81vMGfx7gWf61ncBh6pqI3CoWyfJJmA7cAewFXgoyarlKVeSNKiBAj7J\nOuA3gb/qG94G7O2W9wLv6BvfV1UXqup5YAbYsjzlSpIGdd2A+/0l8IfAjX1jq6vqbLf8IrC6W14L\n/Gfffqe7sf8nyRQw1a2+lORbwDcHrGec3Ip9jZtWe7Ov8fJzSaaqas9SX2DBgE/yW8D5qnoyyV1z\n7VNVlaQW88Zd0T8sPMl0VU0u5jXGgX2Nn1Z7s6/xk2SavpxcrEHO4N8C/HaSe4FXAz+Z5O+Ac0nW\nVNXZJGuA893+Z4D1fcev68YkSdfQgtfgq+rBqlpXVbfT++Ppv1XV7wAHgB3dbjuAR7rlA8D2JDck\n2QBsBA4ve+WSpHkNeg1+Lh8G9ie5H3gBuA+gqo4l2Q8cBy4CO6vq0gCvt+RfQ17h7Gv8tNqbfY2f\noXpL1aIunUuSxoQzWSWpUSse8Em2djNeZ5LsWul6FivJw0nOJznaNzb2s3yTrE/yeJLjSY4leW83\nPta9JXl1ksNJnur6+lA3PtZ9XdbqjPMkJ5N8LcmR7s6SJnpLclOSf0zybJJnkvzKsvZVVSv2AFYB\n/wW8HngV8BSwaSVrWkIPvwq8ETjaN/bnwK5ueRfwZ93ypq7HG4ANXe+rVrqHq/S1Bnhjt3wj8PWu\n/rHuDQjwum75euAJ4M3j3ldff38A/D3waCv/F7t6TwK3XjE29r3RmyT6u93yq4CblrOvlT6D3wLM\nVNU3qur7wD56M2HHRlV9Efj2FcNjP8u3qs5W1Ve65e/R+5iKtYx5b9XzUrd6ffcoxrwv+LGccT7W\nvSX5KXoniJ8CqKrvV9V3WMa+Vjrg1wKn+tbnnPU6huab5Tt2/Sa5HbiT3tnu2PfWXcY4Qm/uxsGq\naqIvXp5x/oO+sRb6gt4P4ceSPNnNgofx720DMAv8dXdZ7a+SvJZl7GulA7551fvdamxvVUryOuCz\nwPuq6rv928a1t6q6VFWb6U3C25LkDVdsH7u++mecX22fceyrz1u7f7PfAHYm+dX+jWPa23X0Lu9+\nvKruBP6H7kMbLxu2r5UO+FZnvZ7rZvcyzrN8k1xPL9w/XVWf64ab6A2g+3X4cXqfejrufV2ecX6S\n3qXOt/XPOIex7QuAqjrTPZ8HPk/v0sS493YaON39Bgnwj/QCf9n6WumA/zKwMcmGJK+iN1P2wArX\ntBzGfpZvktC7NvhMVX20b9NY95ZkIslN3fJrgLcDzzLmfVXDM86TvDbJjZeXgV8HjjLmvVXVi8Cp\nJL/QDd1Nb4Lo8vX1Cvgr8r307tD4L+ADK13PEur/DHAW+F96P5HvB36a3mfkPwc8BtzSt/8Hul5P\nAL+x0vXP09db6f1q+DRwpHvcO+69Ab8IfLXr6yjwJ934WPd1RY938fJdNGPfF7277J7qHscu50Qj\nvW0Gprv/j/8E3LycfTmTVZIatdKXaCRJI2LAS1KjDHhJapQBL0mNMuAlqVEGvCQ1yoCXpEYZ8JLU\nqP8D8hiXtKYHANMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdcbd3adb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) Q-learning: building the network\n",
    "\n",
    "In this section we will build and train naive Q-learning with theano/lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is initializing input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "#create input variables. We'll support multiple states at once\n",
    "\n",
    "\n",
    "current_states = T.matrix(\"states[batch,units]\")\n",
    "actions = T.ivector(\"action_ids[batch]\")\n",
    "rewards = T.vector(\"rewards[batch]\")\n",
    "next_states = T.matrix(\"next states[batch,units]\")\n",
    "is_end = T.ivector(\"vector[batch] where 1 means that session just ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anna/anaconda3/lib/python3.6/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "#input layer\n",
    "l_states = InputLayer((None,)+state_dim)\n",
    "\n",
    "\n",
    "dense_1 = DenseLayer(l_states,num_units=42,\n",
    "                                   name = \"hidden_dense_layer\")\n",
    "\n",
    "\n",
    "#output layer\n",
    "l_qvalues = DenseLayer(dense_1,num_units=n_actions,nonlinearity=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Q-values for `current_states`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get q-values for ALL actions in current_states\n",
    "predicted_qvalues = get_output(l_qvalues,{l_states:current_states})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compiling agent's \"GetQValues\" function\n",
    "get_qvalues = theano.function([current_states], predicted_qvalues,\n",
    "    allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select q-values for chosen actions\n",
    "predicted_qvalues_for_actions = predicted_qvalues[T.arange(actions.shape[0]),actions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and `update`\n",
    "Here we write a function similar to `agent.update`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict q-values for next states\n",
    "predicted_next_qvalues = get_output(l_qvalues,{l_states:next_states})\n",
    "\n",
    "\n",
    "#Computing target q-values under \n",
    "gamma = 0.99\n",
    "target_qvalues_for_actions = rewards + gamma* predicted_next_qvalues.max(axis= 1)\n",
    "    \n",
    "#zero-out q-values at the end\n",
    "target_qvalues_for_actions = (1-is_end)*target_qvalues_for_actions\n",
    "\n",
    "#don't compute gradient over target q-values (consider constant)\n",
    "target_qvalues_for_actions = theano.gradient.disconnected_grad(target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#mean squared error loss function\n",
    "loss = T.mean((predicted_qvalues_for_actions - target_qvalues_for_actions) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all network weights\n",
    "all_weights = get_all_params(l_qvalues,trainable=True)\n",
    "\n",
    "#network updates. Note the small learning rate (for stability)\n",
    "updates = lasagne.updates.sgd(loss,all_weights,learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training function that resembles agent.update(state,action,reward,next_state) \n",
    "#with 1 more argument meaning is_end\n",
    "train_step = theano.function([current_states,actions,rewards,next_states,is_end],\n",
    "    allow_input_downcast=True,\n",
    "                             updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epsilon = 0.25 #initial epsilon\n",
    "\n",
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    \n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #get action q-values from the network\n",
    "        q_values = get_qvalues([s])[0] \n",
    "        \n",
    "        if( np.random.random() < epsilon):\n",
    "            a = np.random.choice(n_actions)\n",
    "        else:\n",
    "            a = np.argmax(q_values)\n",
    "        \n",
    "#         a = <sample action with epsilon-greedy strategy>\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #train agent one step. Note that we use one-element arrays instead of scalars \n",
    "        #because that's what function accepts.\n",
    "        train_step([s],[a],[r],[new_s],[done])\n",
    "        \n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:177.500\tepsilon:0.00141\n",
      "mean reward:185.020\tepsilon:0.00134\n",
      "mean reward:194.370\tepsilon:0.00127\n",
      "mean reward:124.720\tepsilon:0.00121\n",
      "mean reward:116.950\tepsilon:0.00115\n",
      "mean reward:116.660\tepsilon:0.00109\n",
      "mean reward:109.970\tepsilon:0.00103\n",
      "mean reward:142.430\tepsilon:0.00098\n",
      "mean reward:126.940\tepsilon:0.00093\n",
      "mean reward:125.100\tepsilon:0.00089\n",
      "mean reward:118.440\tepsilon:0.00084\n",
      "mean reward:117.700\tepsilon:0.00080\n",
      "mean reward:115.180\tepsilon:0.00076\n",
      "mean reward:112.430\tepsilon:0.00072\n",
      "mean reward:108.860\tepsilon:0.00069\n",
      "mean reward:111.300\tepsilon:0.00065\n",
      "mean reward:108.330\tepsilon:0.00062\n",
      "mean reward:135.340\tepsilon:0.00059\n",
      "mean reward:145.590\tepsilon:0.00056\n",
      "mean reward:142.540\tepsilon:0.00053\n",
      "mean reward:142.660\tepsilon:0.00050\n",
      "mean reward:151.380\tepsilon:0.00048\n",
      "mean reward:164.120\tepsilon:0.00045\n",
      "mean reward:189.960\tepsilon:0.00043\n",
      "mean reward:177.840\tepsilon:0.00041\n",
      "mean reward:173.110\tepsilon:0.00039\n",
      "mean reward:177.510\tepsilon:0.00037\n",
      "mean reward:160.510\tepsilon:0.00035\n",
      "mean reward:122.250\tepsilon:0.00033\n",
      "mean reward:106.850\tepsilon:0.00032\n",
      "mean reward:94.830\tepsilon:0.00030\n",
      "mean reward:86.980\tepsilon:0.00029\n",
      "mean reward:83.050\tepsilon:0.00027\n",
      "mean reward:77.310\tepsilon:0.00026\n",
      "mean reward:74.640\tepsilon:0.00025\n",
      "mean reward:74.680\tepsilon:0.00023\n",
      "mean reward:73.400\tepsilon:0.00022\n",
      "mean reward:73.550\tepsilon:0.00021\n",
      "mean reward:73.230\tepsilon:0.00020\n",
      "mean reward:74.230\tepsilon:0.00019\n",
      "mean reward:74.910\tepsilon:0.00018\n",
      "mean reward:75.610\tepsilon:0.00017\n",
      "mean reward:76.380\tepsilon:0.00016\n",
      "mean reward:76.740\tepsilon:0.00015\n",
      "mean reward:76.910\tepsilon:0.00015\n",
      "mean reward:78.770\tepsilon:0.00014\n",
      "mean reward:77.840\tepsilon:0.00013\n",
      "mean reward:79.330\tepsilon:0.00013\n",
      "mean reward:81.110\tepsilon:0.00012\n",
      "mean reward:80.580\tepsilon:0.00011\n",
      "mean reward:82.820\tepsilon:0.00011\n",
      "mean reward:81.920\tepsilon:0.00010\n",
      "mean reward:84.930\tepsilon:0.00010\n",
      "mean reward:85.020\tepsilon:0.00009\n",
      "mean reward:86.190\tepsilon:0.00009\n",
      "mean reward:86.500\tepsilon:0.00008\n",
      "mean reward:89.040\tepsilon:0.00008\n",
      "mean reward:87.790\tepsilon:0.00008\n",
      "mean reward:89.260\tepsilon:0.00007\n",
      "mean reward:91.120\tepsilon:0.00007\n",
      "mean reward:93.050\tepsilon:0.00006\n",
      "mean reward:93.630\tepsilon:0.00006\n",
      "mean reward:95.740\tepsilon:0.00006\n",
      "mean reward:101.770\tepsilon:0.00006\n",
      "mean reward:102.330\tepsilon:0.00005\n",
      "mean reward:116.700\tepsilon:0.00005\n",
      "mean reward:138.980\tepsilon:0.00005\n",
      "mean reward:162.950\tepsilon:0.00005\n",
      "mean reward:86.650\tepsilon:0.00004\n",
      "mean reward:146.770\tepsilon:0.00004\n",
      "mean reward:163.330\tepsilon:0.00004\n",
      "mean reward:182.120\tepsilon:0.00004\n",
      "mean reward:187.030\tepsilon:0.00004\n",
      "mean reward:116.630\tepsilon:0.00003\n",
      "mean reward:148.750\tepsilon:0.00003\n",
      "mean reward:176.030\tepsilon:0.00003\n",
      "mean reward:166.210\tepsilon:0.00003\n",
      "mean reward:116.030\tepsilon:0.00003\n",
      "mean reward:106.980\tepsilon:0.00003\n",
      "mean reward:90.380\tepsilon:0.00002\n",
      "mean reward:98.990\tepsilon:0.00002\n",
      "mean reward:102.510\tepsilon:0.00002\n",
      "mean reward:100.300\tepsilon:0.00002\n",
      "mean reward:101.280\tepsilon:0.00002\n",
      "mean reward:101.070\tepsilon:0.00002\n",
      "mean reward:101.050\tepsilon:0.00002\n",
      "mean reward:99.540\tepsilon:0.00002\n",
      "mean reward:96.840\tepsilon:0.00002\n",
      "mean reward:97.840\tepsilon:0.00002\n",
      "mean reward:101.000\tepsilon:0.00001\n",
      "mean reward:104.570\tepsilon:0.00001\n",
      "mean reward:109.090\tepsilon:0.00001\n",
      "mean reward:109.840\tepsilon:0.00001\n",
      "mean reward:112.910\tepsilon:0.00001\n",
      "mean reward:117.080\tepsilon:0.00001\n",
      "mean reward:116.210\tepsilon:0.00001\n",
      "mean reward:118.570\tepsilon:0.00001\n",
      "mean reward:116.340\tepsilon:0.00001\n",
      "mean reward:109.030\tepsilon:0.00001\n",
      "mean reward:116.690\tepsilon:0.00001\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    \n",
    "    rewards = [generate_session() for j in range(100)] #generate new sessions\n",
    "    \n",
    "    epsilon*=0.95\n",
    "    \n",
    "    print (\"mean reward:%.3f\\tepsilon:%.5f\"%(np.mean(rewards),epsilon))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "        \n",
    "    assert epsilon!=0, \"Please explore environment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon=0 #Don't forget to reset epsilon back to initial value if you want to go on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#unwrap \n",
    "env = env.env.env\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") #you'll need me later\n",
    "\n",
    "#Warning! If you keep seeing error that reads something like\"DoubleWrapError\",\n",
    "#run env=gym.make(\"CartPole-v0\");env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
